{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-08 11:51:59.491806: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "import shutil\n",
    "shutil.rmtree('logs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import VGG16 pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-08 11:52:01.769941: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-08 11:52:02.363461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1637] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7910 MB memory:  -> device: 0, name: A100-SXM4-40GB MIG 2g.10gb, pci bus id: 0000:90:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers:  19\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
    "\n",
    "batch_size = 16\n",
    "img_height = 184\n",
    "img_width = 216\n",
    "img_size = (img_height, img_width)\n",
    "img_shape = img_size + (3,)\n",
    "\n",
    "with tf.device(\"CPU\"):\n",
    "    pre_trained_model = VGG16(input_shape = img_shape,\n",
    "                            include_top = False,\n",
    "                            weights = 'imagenet')\n",
    "\n",
    "    pre_trained_model.trainable = True\n",
    "print(\"Number of layers: \", len(pre_trained_model.layers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21413 files belonging to 3 classes.\n",
      "Using 17131 files for training.\n",
      "Found 21413 files belonging to 3 classes.\n",
      "Using 4282 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# Edit according to local path for dataset\n",
    "ds_path = r\"/drive0-storage/Gracia/dataset\"\n",
    "\n",
    "with tf.device(\"CPU\"):\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(ds_path,\n",
    "                                            validation_split = 0.2,\n",
    "                                            subset = \"training\",\n",
    "                                            seed = 123,\n",
    "                                            image_size = img_size,\n",
    "                                            batch_size = batch_size)\n",
    "\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(ds_path,\n",
    "                                          validation_split = 0.2,\n",
    "                                          subset = \"validation\",\n",
    "                                          seed = 123,\n",
    "                                          image_size = img_size,\n",
    "                                          batch_size = batch_size)\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "num_classes = len(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hyperparameter setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([1e-5, 1e-4, 1e-3, 1e-2]))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.2, 0.4))\n",
    "# HP_TUNING_LAYER = hp.HParam('tuning_layers', hp.IntInterval(165, 170))\n",
    "# HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
    "\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_LEARNING_RATE, HP_DROPOUT],\n",
    "        metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training & run function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(hparams):\n",
    "    # Averaging layer\n",
    "    global_average = tf.keras.layers.GlobalAveragePooling2D()\n",
    "\n",
    "    # Add dense layer\n",
    "    prediction_layer = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "    # Chain model \n",
    "    inputs = tf.keras.Input(shape = img_shape)\n",
    "    x = preprocess_input(inputs)\n",
    "    x = pre_trained_model(x, training=False)\n",
    "    x = global_average(x)\n",
    "    x = tf.keras.layers.Dropout(hparams[HP_DROPOUT])(x)\n",
    "    outputs = prediction_layer(x)\n",
    "    model = tf.keras.Model(inputs,outputs)\n",
    "    \n",
    "    # Compile model\n",
    "    LR = hparams[HP_LEARNING_RATE]\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = LR),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits = False),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Train model\n",
    "    EPOCH = 15\n",
    "\n",
    "    model.fit(train_ds,\n",
    "        validation_data = val_ds,\n",
    "        epochs = EPOCH,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.TensorBoard(logdir),  \n",
    "            hp.KerasCallback(logdir, hparams)\n",
    "            ]\n",
    "        )\n",
    "    _, accuracy = model.evaluate(val_ds)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training_SGD(hparams):\n",
    "    # Averaging layer\n",
    "    global_average = tf.keras.layers.GlobalAveragePooling2D()\n",
    "\n",
    "    # Add dense layer\n",
    "    prediction_layer = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "    # Chain model \n",
    "    inputs = tf.keras.Input(shape = img_shape)\n",
    "    x = preprocess_input(inputs)\n",
    "    x = pre_trained_model(x, training=False)\n",
    "    x = global_average(x)\n",
    "    x = tf.keras.layers.Dropout(hparams[HP_DROPOUT])(x)\n",
    "    outputs = prediction_layer(x)\n",
    "    model = tf.keras.Model(inputs,outputs)\n",
    "    \n",
    "    # Compile model\n",
    "    LR = hparams[HP_LEARNING_RATE]\n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate = LR),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits = False),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Train model\n",
    "    EPOCH = 15\n",
    "\n",
    "    model.fit(train_ds,\n",
    "        validation_data = val_ds,\n",
    "        epochs = EPOCH,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.TensorBoard(logdir),  \n",
    "            hp.KerasCallback(logdir, hparams)\n",
    "            ]\n",
    "        )\n",
    "    _, accuracy = model.evaluate(val_ds)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)\n",
    "        accuracy = model_training(hparams)\n",
    "        tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-0\n",
      "{'learning_rate': 1e-05, 'dropout': 0.2}\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-08 11:54:18.891369: I tensorflow/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-06-08 11:54:19.962444: I tensorflow/stream_executor/cuda/cuda_blas.cc:1633] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1071/1071 [==============================] - 115s 104ms/step - loss: 0.9781 - accuracy: 0.5575 - val_loss: 0.8929 - val_accuracy: 0.5831\n",
      "Epoch 2/15\n",
      "1071/1071 [==============================] - 110s 103ms/step - loss: 0.8075 - accuracy: 0.6367 - val_loss: 0.7203 - val_accuracy: 0.6714\n",
      "Epoch 3/15\n",
      "1071/1071 [==============================] - 110s 102ms/step - loss: 0.6010 - accuracy: 0.7468 - val_loss: 0.6153 - val_accuracy: 0.7506\n",
      "Epoch 4/15\n",
      "1071/1071 [==============================] - 110s 102ms/step - loss: 0.3752 - accuracy: 0.8518 - val_loss: 0.4165 - val_accuracy: 0.8375\n",
      "Epoch 5/15\n",
      "1071/1071 [==============================] - 110s 102ms/step - loss: 0.2197 - accuracy: 0.9183 - val_loss: 0.3249 - val_accuracy: 0.8860\n",
      "Epoch 6/15\n",
      "1071/1071 [==============================] - 110s 102ms/step - loss: 0.1293 - accuracy: 0.9552 - val_loss: 0.2671 - val_accuracy: 0.8989\n",
      "Epoch 7/15\n",
      "1071/1071 [==============================] - 109s 102ms/step - loss: 0.0907 - accuracy: 0.9681 - val_loss: 0.1570 - val_accuracy: 0.9475\n",
      "Epoch 8/15\n",
      "1071/1071 [==============================] - 110s 102ms/step - loss: 0.0763 - accuracy: 0.9722 - val_loss: 0.2085 - val_accuracy: 0.9229\n",
      "Epoch 9/15\n",
      "1071/1071 [==============================] - 110s 102ms/step - loss: 0.0543 - accuracy: 0.9820 - val_loss: 0.4268 - val_accuracy: 0.8713\n",
      "Epoch 10/15\n",
      "1071/1071 [==============================] - 110s 102ms/step - loss: 0.0492 - accuracy: 0.9840 - val_loss: 0.1405 - val_accuracy: 0.9526\n",
      "Epoch 11/15\n",
      "1071/1071 [==============================] - 110s 102ms/step - loss: 0.0512 - accuracy: 0.9828 - val_loss: 0.1344 - val_accuracy: 0.9521\n",
      "Epoch 12/15\n",
      "1071/1071 [==============================] - 110s 102ms/step - loss: 0.0350 - accuracy: 0.9887 - val_loss: 0.1716 - val_accuracy: 0.9418\n",
      "Epoch 13/15\n",
      "1071/1071 [==============================] - 110s 102ms/step - loss: 0.0355 - accuracy: 0.9890 - val_loss: 0.1305 - val_accuracy: 0.9563\n",
      "Epoch 14/15\n",
      "1071/1071 [==============================] - 110s 102ms/step - loss: 0.0361 - accuracy: 0.9883 - val_loss: 0.0909 - val_accuracy: 0.9675\n",
      "Epoch 15/15\n",
      "1071/1071 [==============================] - 109s 102ms/step - loss: 0.0329 - accuracy: 0.9885 - val_loss: 0.4064 - val_accuracy: 0.8526\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.4064 - accuracy: 0.8526\n",
      "--- Starting trial: run-1\n",
      "{'learning_rate': 1e-05, 'dropout': 0.30000000000000004}\n",
      "Epoch 1/15\n",
      "1071/1071 [==============================] - 110s 102ms/step - loss: 0.1997 - accuracy: 0.9254 - val_loss: 0.1580 - val_accuracy: 0.9477\n",
      "Epoch 2/15\n",
      "1071/1071 [==============================] - 109s 102ms/step - loss: 0.0597 - accuracy: 0.9808 - val_loss: 0.0992 - val_accuracy: 0.9682\n",
      "Epoch 3/15\n",
      "1071/1071 [==============================] - 109s 102ms/step - loss: 0.0473 - accuracy: 0.9844 - val_loss: 0.1347 - val_accuracy: 0.9545\n",
      "Epoch 4/15\n",
      "1071/1071 [==============================] - 109s 102ms/step - loss: 0.0393 - accuracy: 0.9872 - val_loss: 0.1498 - val_accuracy: 0.9437\n",
      "Epoch 5/15\n",
      " 724/1071 [===================>..........] - ETA: 32s - loss: 0.0367 - accuracy: 0.9892"
     ]
    }
   ],
   "source": [
    "session_num = 0\n",
    "logdir = r'logs/hparam_tuning'\n",
    "\n",
    "for learning_rate in HP_LEARNING_RATE.domain.values:\n",
    "    for dropout_rate in np.linspace(HP_DROPOUT.domain.min_value,\n",
    "                                    HP_DROPOUT.domain.max_value,\n",
    "                                    3):\n",
    "#         for tuning_layer in (HP_TUNING_LAYER.domain.min_value,\n",
    "#                              HP_TUNING_LAYER.domain.max_value):\n",
    "#                             np.linspace(HP_TUNING_LAYER.domain.min_value,\n",
    "#                                         HP_TUNING_LAYER.domain.max_value,\n",
    "#                                         6, dtype='int'):\n",
    "        hparams = {\n",
    "                  HP_LEARNING_RATE: learning_rate,\n",
    "                  HP_DROPOUT: dropout_rate\n",
    "#                   HP_TUNING_LAYER: tuning_layer,\n",
    "                  }\n",
    "        run_name = \"run-%d\" % session_num\n",
    "        print('--- Starting trial: %s' % run_name)\n",
    "        print({h.name: hparams[h] for h in hparams})\n",
    "        run('logs/hparam_tuning/' + run_name, hparams)\n",
    "        session_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize in TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-11480cff34d33599\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-11480cff34d33599\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/hparam_tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
