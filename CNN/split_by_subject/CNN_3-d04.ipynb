{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d915822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cea715",
   "metadata": {},
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefe5611",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "938cd0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 184\n",
    "img_width = 216\n",
    "img_size = (img_height, img_width)\n",
    "img_shape = img_size + (3,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7d80b3",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b79ccd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34130 files belonging to 3 classes.\n",
      "Found 4282 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "\n",
    "train_path = \"./dataset_3/training\"\n",
    "val_path = \"./dataset_3/validation\"\n",
    "\n",
    "with tf.device(\"CPU\"):\n",
    "    train_ds = image_dataset_from_directory(train_path,\n",
    "                                        seed = 123,\n",
    "                                        image_size = img_size,\n",
    "                                        batch_size = batch_size)\n",
    "    \n",
    "    val_ds = image_dataset_from_directory(val_path,\n",
    "                                      seed = 456,\n",
    "                                      image_size = img_size,\n",
    "                                      batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8639c23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.rmdir(\"/drive0-storage/Gracia/dataset_1/.ipynb_checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "667b1759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bipolar_disorder', 'healthy_controls', 'schizophrenia']\n"
     ]
    }
   ],
   "source": [
    "class_names = train_ds.class_names\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a6d8f5",
   "metadata": {},
   "source": [
    "### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6be0866",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"CPU\"):\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    val_batches = tf.data.experimental.cardinality(val_ds)\n",
    "    test_dataset = val_ds.take(val_batches // 2)\n",
    "    validation_dataset = val_ds.skip(val_batches // 2)\n",
    "    \n",
    "    # Buffered prefetching\n",
    "    train_dataset = train_ds.prefetch(buffer_size = AUTOTUNE)\n",
    "    validation_dataset = validation_dataset.prefetch(buffer_size = AUTOTUNE)\n",
    "    test_dataset = test_dataset.prefetch(buffer_size = AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87a2c977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n",
      "67\n"
     ]
    }
   ],
   "source": [
    "print(len(validation_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7c2699",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6b3d807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vgg16(dropout_num):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Block 1\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=img_shape))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Dropout(dropout_num))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(Dropout(dropout_num))\n",
    "\n",
    "    # Block 2\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Dropout(dropout_num))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Block 3\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Dropout(dropout_num))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Block 4\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Dropout(dropout_num))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Block 5\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Dropout(dropout_num))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Flatten the output and create fully connected layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(dropout_num))\n",
    "    model.add(Dense(3, activation='softmax'))  \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1602c835",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"GPU\"):\n",
    "    model = build_vgg16(0.2)\n",
    "\n",
    "    # compile model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 1e-5),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits = False),\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca96214",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9f77a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback function\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('accuracy')>0.99):\n",
    "            print(\"\\nReached 99% accuracy, cancelling training\")\n",
    "            self.model.stop_training = True\n",
    "            \n",
    "callbacks = myCallback()\n",
    "save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n",
    "checkpoints_cb = tf.keras.callbacks.ModelCheckpoint('./cnn1_checkpoints', options=save_locally)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03333c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "   6/1067 [..............................] - ETA: 3:17 - loss: 1.1135 - accuracy: 0.4427WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0635s vs `on_train_batch_end` time: 0.1240s). Check your callbacks.\n",
      "1067/1067 [==============================] - 204s 191ms/step - loss: 0.8926 - accuracy: 0.5796 - val_loss: 1.0363 - val_accuracy: 0.4986\n",
      "Epoch 2/30\n",
      "1067/1067 [==============================] - 204s 191ms/step - loss: 0.5868 - accuracy: 0.7503 - val_loss: 1.0809 - val_accuracy: 0.4238\n",
      "Epoch 3/30\n",
      "1067/1067 [==============================] - 203s 190ms/step - loss: 0.4872 - accuracy: 0.7888 - val_loss: 1.0281 - val_accuracy: 0.4897\n",
      "Epoch 4/30\n",
      "1067/1067 [==============================] - 203s 190ms/step - loss: 0.4301 - accuracy: 0.8143 - val_loss: 1.1177 - val_accuracy: 0.3980\n",
      "Epoch 5/30\n",
      "1067/1067 [==============================] - 203s 190ms/step - loss: 0.3791 - accuracy: 0.8374 - val_loss: 1.0967 - val_accuracy: 0.3915\n",
      "Epoch 6/30\n",
      "1067/1067 [==============================] - 203s 190ms/step - loss: 0.3297 - accuracy: 0.8612 - val_loss: 1.1088 - val_accuracy: 0.4210\n",
      "Epoch 7/30\n",
      "1067/1067 [==============================] - 203s 190ms/step - loss: 0.2794 - accuracy: 0.8846 - val_loss: 1.0856 - val_accuracy: 0.4579\n",
      "Epoch 8/30\n",
      "1067/1067 [==============================] - 203s 190ms/step - loss: 0.2302 - accuracy: 0.9075 - val_loss: 1.0813 - val_accuracy: 0.4486\n",
      "Epoch 9/30\n",
      "1067/1067 [==============================] - 203s 190ms/step - loss: 0.1790 - accuracy: 0.9287 - val_loss: 1.2099 - val_accuracy: 0.4186\n",
      "Epoch 10/30\n",
      "1067/1067 [==============================] - 203s 190ms/step - loss: 0.1507 - accuracy: 0.9406 - val_loss: 1.2117 - val_accuracy: 0.4415\n",
      "Epoch 11/30\n",
      "1067/1067 [==============================] - 203s 190ms/step - loss: 0.1165 - accuracy: 0.9551 - val_loss: 1.1605 - val_accuracy: 0.4584\n",
      "Epoch 12/30\n",
      "1067/1067 [==============================] - 203s 190ms/step - loss: 0.0986 - accuracy: 0.9620 - val_loss: 1.2376 - val_accuracy: 0.4701\n",
      "Epoch 13/30\n",
      "1067/1067 [==============================] - 203s 190ms/step - loss: 0.0838 - accuracy: 0.9684 - val_loss: 1.3012 - val_accuracy: 0.4518\n",
      "Epoch 14/30\n",
      "1067/1067 [==============================] - 203s 190ms/step - loss: 0.0707 - accuracy: 0.9738 - val_loss: 1.3167 - val_accuracy: 0.4523\n",
      "Epoch 15/30\n",
      "1067/1067 [==============================] - 203s 190ms/step - loss: 0.0652 - accuracy: 0.9756 - val_loss: 1.4023 - val_accuracy: 0.4060\n",
      "Epoch 16/30\n",
      "1067/1067 [==============================] - 203s 190ms/step - loss: 0.0545 - accuracy: 0.9799 - val_loss: 1.2966 - val_accuracy: 0.4836\n",
      "Epoch 17/30\n",
      "1067/1067 [==============================] - 203s 190ms/step - loss: 0.0519 - accuracy: 0.9804 - val_loss: 1.4028 - val_accuracy: 0.4766\n",
      "Epoch 18/30\n",
      "1067/1067 [==============================] - 203s 190ms/step - loss: 0.0481 - accuracy: 0.9831 - val_loss: 1.4119 - val_accuracy: 0.4560\n",
      "Epoch 19/30\n",
      "1067/1067 [==============================] - 203s 190ms/step - loss: 0.0386 - accuracy: 0.9854 - val_loss: 1.4446 - val_accuracy: 0.4383\n",
      "Epoch 20/30\n",
      "1067/1067 [==============================] - 203s 190ms/step - loss: 0.0395 - accuracy: 0.9857 - val_loss: 1.4155 - val_accuracy: 0.4238\n",
      "Epoch 21/30\n",
      "1067/1067 [==============================] - 203s 191ms/step - loss: 0.0355 - accuracy: 0.9877 - val_loss: 1.6155 - val_accuracy: 0.4574\n",
      "Epoch 22/30\n",
      " 252/1067 [======>.......................] - ETA: 2:31 - loss: 0.0289 - accuracy: 0.9887"
     ]
    }
   ],
   "source": [
    "EPOCH = 30\n",
    "\n",
    "history = model.fit(\n",
    "            train_dataset,\n",
    "            validation_data = validation_dataset,\n",
    "            epochs = EPOCH,\n",
    "            batch_size = 32,\n",
    "            callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f080676",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/cnn1_14epoch.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fee2451",
   "metadata": {},
   "source": [
    "### Training evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15eedbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# training and validation accuracy\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "# plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Accuracy', size=15, fontweight='bold')\n",
    "\n",
    "# training and validation loss\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.xlabel('Epoch')\n",
    "# plt.ylim([0,1.0])\n",
    "plt.title('Loss', size=15, fontweight='bold')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff8d40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61163d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b53006",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb81e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e344c6ff",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1af5912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6238ca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(test_dataset)\n",
    "dict(zip(model.metrics_names, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96002f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = []  # predicted labels\n",
    "true = []  # true labels\n",
    "\n",
    "for image_batch, label_batch in test_dataset:  \n",
    "    true.append(label_batch)\n",
    "    prediction = model.predict(image_batch, verbose=0)\n",
    "    predicted.append(np.argmax(prediction, axis=-1))\n",
    "\n",
    "# convert labels into tensors\n",
    "true_labels = tf.concat([item for item in true], axis=0)\n",
    "predicted_labels = tf.concat([item for item in predicted], axis=0)\n",
    "\n",
    "cf_matrix = confusion_matrix(true_labels, predicted_labels, normalize='true')\n",
    "\n",
    "# plot confusion  matrix\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot(111)\n",
    "sns.heatmap(cf_matrix, \n",
    "            annot=True)\n",
    "ax.xaxis.set_ticklabels(class_names)\n",
    "ax.yaxis.set_ticklabels(class_names)\n",
    "plt.xlabel('Predicted Label', labelpad=15, size=12, fontweight='bold')\n",
    "plt.ylabel('True Label', labelpad=15, size=12, fontweight='bold')\n",
    "plt.title('Confusion Matrix', size=15, fontweight='bold')\n",
    "plt.savefig('./CNN1_cm.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92ba09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
    "sensitivity = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "specificity = []\n",
    "for i in range(num_classes):\n",
    "    true_negatives = np.sum(np.delete(np.delete(cf_matrix, i, axis=0), i, axis=1))\n",
    "    false_positives = np.sum(cf_matrix[:, i]) - cf_matrix[i, i]\n",
    "    specificity.append(true_negatives / (true_negatives + false_positives))\n",
    "\n",
    "# Calculate average specificity\n",
    "average_specificity = np.mean(specificity)\n",
    "\n",
    "print(\"Accuracy:\", result[1])\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "print(\"Specificity:\", average_specificity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
