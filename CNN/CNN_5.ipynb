{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8008d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c953835",
   "metadata": {},
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c866f382",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690dc8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "img_height = 184\n",
    "img_width = 216\n",
    "img_size = (img_height, img_width)\n",
    "img_shape = img_size + (3,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c3866c",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a34349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "\n",
    "train_path = \"/drive0-storage/Gracia/dataset_5/training\"\n",
    "val_path = \"/drive0-storage/Gracia/dataset_5/validation\"\n",
    "\n",
    "with tf.device(\"CPU\"):\n",
    "    train_ds = image_dataset_from_directory(train_path,\n",
    "                                        seed = 123,\n",
    "                                        image_size = img_size,\n",
    "                                        batch_size = batch_size)\n",
    "    \n",
    "    val_ds = image_dataset_from_directory(val_path,\n",
    "                                      seed = 456,\n",
    "                                      image_size = img_size,\n",
    "                                      batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1208ca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.rmdir(\"/drive0-storage/Gracia/dataset_1/.ipynb_checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148b3c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_ds.class_names\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73d9413",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"CPU\"):\n",
    "    plt.figure(figsize=(7,5))\n",
    "    for images, labels in train_ds.take(1):\n",
    "        org_image = images[0]\n",
    "        for i in range(9):\n",
    "            ax = plt.subplot(3, 3, i + 1)\n",
    "            plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "    print(org_image.dtype)\n",
    "    print(np.min(org_image), np.max(org_image))\n",
    "    print(org_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa46553c",
   "metadata": {},
   "source": [
    "### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2d5f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"CPU\"):\n",
    "    val_batches = tf.data.experimental.cardinality(val_ds)\n",
    "    test_dataset = val_ds.take(val_batches // 2)\n",
    "    validation_dataset = val_ds.skip(val_batches // 2)\n",
    "    \n",
    "    # Buffered prefetching\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    train_dataset = train_ds.prefetch(buffer_size = AUTOTUNE)\n",
    "    validation_dataset = validation_dataset.prefetch(buffer_size = AUTOTUNE)\n",
    "    test_dataset = test_dataset.prefetch(buffer_size = AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a84d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(validation_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c394951b",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88011c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vgg16():\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Block 1\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=img_shape))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Block 2\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Block 3\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Block 4\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Block 5\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Flatten the output and create fully connected layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.4))\n",
    "    model.add(Dense(num_classes, activation='softmax'))  \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1d99d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"GPU\"):\n",
    "    vgg16 = build_vgg16()\n",
    "\n",
    "    # add rescale layer, chain model\n",
    "    rescale = tf.keras.layers.Rescaling(1./255)\n",
    "    inputs = tf.keras.Input(shape = img_shape)\n",
    "    x = rescale(inputs)\n",
    "    outputs = vgg16(x)\n",
    "    model = tf.keras.Model(inputs,outputs) \n",
    "    \n",
    "    # compile model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 1e-5),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits = False),\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082fa794",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aadf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback function\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('accuracy')>0.98):\n",
    "            print(\"\\nReached 98% accuracy, cancelling training\")\n",
    "            self.model.stop_training = True\n",
    "            \n",
    "callbacks = myCallback()\n",
    "\n",
    "save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n",
    "checkpoints_cb = tf.keras.callbacks.ModelCheckpoint('./checkpoints', options=save_locally)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fd6eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 50\n",
    "\n",
    "history = model.fit(\n",
    "            train_dataset,\n",
    "            validation_data = validation_dataset,\n",
    "            epochs = EPOCH,\n",
    "            callbacks=[checkpoints_cb, early_stopping, callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b3fd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 20\n",
    "\n",
    "history_no_callback = model.fit(\n",
    "            train_dataset,\n",
    "            validation_data = validation_dataset,\n",
    "            epochs = EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2762a4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/cnn5_20epoch.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbd3644",
   "metadata": {},
   "source": [
    "### Training evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f635f646",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# training and validation accuracy\n",
    "acc = history_no_callback.history['accuracy']\n",
    "val_acc = history_no_callback.history['val_accuracy']\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Accuracy', size=15, fontweight='bold')\n",
    "\n",
    "# training and validation loss\n",
    "loss = history_no_callback.history['loss']\n",
    "val_loss = history_no_callback.history['val_loss']\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.xlabel('Epoch')\n",
    "# plt.ylim([0,1.0])\n",
    "plt.title('Loss', size=15, fontweight='bold')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8511af61",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0efe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b821f5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(test_dataset)\n",
    "dict(zip(model.metrics_names, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9219dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = []  # predicted labels\n",
    "true = []  # true labels\n",
    "\n",
    "for image_batch, label_batch in test_dataset:  \n",
    "    true.append(label_batch)\n",
    "    prediction = model.predict(image_batch, verbose=0)\n",
    "    predicted.append(np.argmax(prediction, axis=-1))\n",
    "\n",
    "# convert labels into tensors\n",
    "true_labels = tf.concat([item for item in true], axis=0)\n",
    "predicted_labels = tf.concat([item for item in predicted], axis=0)\n",
    "\n",
    "cf_matrix = confusion_matrix(true_labels, predicted_labels, normalize='true')\n",
    "\n",
    "# plot confusion  matrix\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(cf_matrix, \n",
    "            annot=True)\n",
    "plt.title('Confusion Matrix', size=15, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04256a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
    "sensitivity = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "specificity = []\n",
    "for i in range(num_classes):\n",
    "    true_negatives = np.sum(np.delete(np.delete(cf_matrix, i, axis=0), i, axis=1))\n",
    "    false_positives = np.sum(cf_matrix[:, i]) - cf_matrix[i, i]\n",
    "    specificity.append(true_negatives / (true_negatives + false_positives))\n",
    "\n",
    "# Calculate average specificity\n",
    "average_specificity = np.mean(specificity)\n",
    "\n",
    "print(\"Accuracy:\", result[1])\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "print(\"Specificity:\", average_specificity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
